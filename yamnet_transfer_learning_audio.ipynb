{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudyhendrawn/Riset-AI-SIKGM/blob/main/yamnet_transfer_learning_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjZQV2njKJ3U"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL0TERThT6z"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/audio/transfer_learning_audio.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2madPFAGHb3"
      },
      "source": [
        "# Transfer learning with YAMNet for environmental sound classification\n",
        "\n",
        "[YAMNet](https://tfhub.dev/google/yamnet/1) is a pre-trained deep neural network that can predict audio events from [521 classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv), such as laughter, barking, or a siren. \n",
        "\n",
        " In this tutorial you will learn how to:\n",
        "\n",
        "- Load and use the YAMNet model for inference.\n",
        "- Build a new model using the YAMNet embeddings to classify cat and dog sounds.\n",
        "- Evaluate and export your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mdp2TpBh96Y"
      },
      "source": [
        "## Import TensorFlow and other libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCcKYqu_hvKe"
      },
      "source": [
        "Start by installing [TensorFlow I/O](https://www.tensorflow.org/io), which will make it easier for you to load audio files off disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBpRWDHTHHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85983bd6-b4fd-4137-91fb-a54b65cf7a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"tensorflow==2.11.*\"\n",
        "# tensorflow_io 0.28 is compatible with TensorFlow 2.11\n",
        "!pip install -q \"tensorflow_io==0.28.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l3nqdWVF-kC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "from IPython import display\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ZhybCnt_bM"
      },
      "source": [
        "## About YAMNet\n",
        "\n",
        "[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) is a pre-trained neural network that employs the [MobileNetV1](https://arxiv.org/abs/1704.04861) depthwise-separable convolution architecture. It can use an audio waveform as input and make independent predictions for each of the 521 audio events from the [AudioSet](http://g.co/audioset) corpus.\n",
        "\n",
        "Internally, the model extracts \"frames\" from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds .\n",
        "\n",
        "The model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range `[-1.0, +1.0]`. This tutorial contains code to help you convert WAV files into the supported format.\n",
        "\n",
        "The model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel [spectrogram](https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram). You can find more details [here](https://tfhub.dev/google/yamnet/1).\n",
        "\n",
        "One specific use of YAMNet is as a high-level feature extractor - the 1,024-dimensional embedding output. You will use the base (YAMNet) model's input features and feed them into your shallower model consisting of one hidden `tf.keras.layers.Dense` layer. Then, you will train the network on a small amount of data for audio classification _without_ requiring a lot of labeled data and training end-to-end. (This is similar to [transfer learning for image classification with TensorFlow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) for more information.)\n",
        "\n",
        "First, you will test the model and see the results of classifying audio. You will then construct the data pre-processing pipeline.\n",
        "\n",
        "### Loading YAMNet from TensorFlow Hub\n",
        "\n",
        "You are going to use a pre-trained YAMNet from [Tensorflow Hub](https://tfhub.dev/) to extract the embeddings from the sound files.\n",
        "\n",
        "Loading a model from TensorFlow Hub is straightforward: choose the model, copy its URL, and use the `load` function.\n",
        "\n",
        "Note: to read the documentation of the model, use the model URL in your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CWkBV5v3gr"
      },
      "outputs": [],
      "source": [
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBm9y9iV2U_-"
      },
      "source": [
        "You will need a function to load audio files, which will also be used later when working with the training data. (Learn more about reading audio files and their labels in [Simple audio recognition](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels).\n",
        "\n",
        "Note: The returned `wav_data` from `load_wav_16k_mono` is already normalized to values in the `[-1.0, 1.0]` range (for more information, go to [YAMNet's documentation on TF Hub](https://tfhub.dev/google/yamnet/1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwc9Wrdg2EtY"
      },
      "outputs": [],
      "source": [
        "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yKrH711wA40"
      },
      "source": [
        "## Bioakustik Gajah Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkrR1AWbwFGn",
        "outputId": "31a76a8b-47d2-4016-d2ef-20d0a230a9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZOOKxBc8Wg"
      },
      "source": [
        "### Use this if the dataset still in Zip format\n",
        "\n",
        "Otherwise, skip it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNmNwrK_wFEK"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/datasets/bioakustik-gajah'\n",
        "# FILENAME = 'segmented_audio_data_v2.zip'\n",
        "\n",
        "# from zipfile import ZipFile\n",
        "  \n",
        "# # loading the temp.zip and creating a zip object\n",
        "# with ZipFile(os.path.join(DATA_DIR, FILENAME), 'r') as zObject:\n",
        "#     zObject.extractall(path=DATA_DIR)\n",
        "# zObject.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "G4O_SnLpwFBv",
        "outputId": "048b54e1-88b1-4b6f-cb5c-97586b43f7ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                filename   sound_type  sample_rate  num_frames  num_channels  \\\n",
              "1478       rumble_18.wav       rumble        22050       68782             1   \n",
              "1479  firecracker_64.wav  firecracker        22050       16777             1   \n",
              "1480     trumpet_137.wav      trumpet        22050       37587             1   \n",
              "1481  roar_rumble_48.wav  roar_rumble        22050       55570             1   \n",
              "1482      rumble_350.wav       rumble        22050       62646             1   \n",
              "\n",
              "      bits_per_sample encoding  class_id    alt_class  alt_class_id  \n",
              "1478               16    PCM_S         2       rumble             2  \n",
              "1479               16    PCM_S        17        other             1  \n",
              "1480               16    PCM_S         0      trumpet             0  \n",
              "1481               16    PCM_S         7  roar_rumble             4  \n",
              "1482               16    PCM_S         2       rumble             2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-770b7b17-2094-4f06-a17a-8d362aa7ac7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>sound_type</th>\n",
              "      <th>sample_rate</th>\n",
              "      <th>num_frames</th>\n",
              "      <th>num_channels</th>\n",
              "      <th>bits_per_sample</th>\n",
              "      <th>encoding</th>\n",
              "      <th>class_id</th>\n",
              "      <th>alt_class</th>\n",
              "      <th>alt_class_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>rumble_18.wav</td>\n",
              "      <td>rumble</td>\n",
              "      <td>22050</td>\n",
              "      <td>68782</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>PCM_S</td>\n",
              "      <td>2</td>\n",
              "      <td>rumble</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>firecracker_64.wav</td>\n",
              "      <td>firecracker</td>\n",
              "      <td>22050</td>\n",
              "      <td>16777</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>PCM_S</td>\n",
              "      <td>17</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>trumpet_137.wav</td>\n",
              "      <td>trumpet</td>\n",
              "      <td>22050</td>\n",
              "      <td>37587</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>PCM_S</td>\n",
              "      <td>0</td>\n",
              "      <td>trumpet</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>roar_rumble_48.wav</td>\n",
              "      <td>roar_rumble</td>\n",
              "      <td>22050</td>\n",
              "      <td>55570</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>PCM_S</td>\n",
              "      <td>7</td>\n",
              "      <td>roar_rumble</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1482</th>\n",
              "      <td>rumble_350.wav</td>\n",
              "      <td>rumble</td>\n",
              "      <td>22050</td>\n",
              "      <td>62646</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>PCM_S</td>\n",
              "      <td>2</td>\n",
              "      <td>rumble</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-770b7b17-2094-4f06-a17a-8d362aa7ac7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-770b7b17-2094-4f06-a17a-8d362aa7ac7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-770b7b17-2094-4f06-a17a-8d362aa7ac7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_metadata = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
        "val_metadata = pd.read_csv(os.path.join(DATA_DIR, 'val.csv'))\n",
        "test_metadata = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
        "train_metadata.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owyBtPhs10g1",
        "outputId": "71492564-202d-4979-f617-5cf426ce8d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation length: 371\n",
            "Test length: 206\n"
          ]
        }
      ],
      "source": [
        "print(\"Validation length: {}\".format(len(val_metadata)))\n",
        "print(\"Test length: {}\".format(len(test_metadata)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQku2i3vwE_N",
        "outputId": "8797322c-40a7-42a2-e109-d41ace2c10e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
              "1    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
              "2    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
              "3    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
              "4    /content/drive/MyDrive/Colab Notebooks/dataset...\n",
              "Name: filename, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Give complete path to filename\n",
        "train_metadata['filename'] = train_metadata['filename'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
        "val_metadata['filename'] = val_metadata['filename'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
        "test_metadata['filename'] = test_metadata['filename'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
        "\n",
        "# Use only filename and alt_class_id column in train, val, and test\n",
        "train_df_filename = train_metadata['filename'].copy()\n",
        "train_df_label = train_metadata['alt_class_id'].copy()\n",
        "\n",
        "val_df_filename = val_metadata['filename'].copy()\n",
        "val_df_label = val_metadata['alt_class_id'].copy()\n",
        "\n",
        "test_df_filename = test_metadata['filename'].copy()\n",
        "test_df_label = test_metadata['alt_class_id'].copy()\n",
        "\n",
        "train_df_filename.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSb9b9nehrSe",
        "outputId": "188d20e9-3f2e-4437-b2b0-3692c5ec3399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    527\n",
              "1    370\n",
              "0    292\n",
              "5    173\n",
              "3     72\n",
              "4     39\n",
              "7      6\n",
              "6      4\n",
              "Name: alt_class_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_df_label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkDcBS-aJdCz"
      },
      "source": [
        "### Load the audio files and retrieve embeddings\n",
        "\n",
        "Here you'll apply the `load_wav_16k_mono` and prepare the WAV data for the model.\n",
        "\n",
        "When extracting embeddings from the WAV data, you get an array of shape `(N, 1024)`  where `N` is the number of frames that YAMNet found (one for every 0.48 seconds of audio)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKDT5RomaDKO"
      },
      "source": [
        "Your model will use each frame as one input. Therefore, you need to create a new column that has one frame per row. You also need to expand the labels and the `fold` column to proper reflect these new rows.\n",
        "\n",
        "The expanded `fold` column keeps the original values. You cannot mix frames because, when performing the splits, you might end up having parts of the same audio on different splits, which would make your validation and test steps less effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Rq3_PyKLtU",
        "outputId": "9941513f-6583-4a29-a48c-c6f7317f0797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_df_filename, train_df_label))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_df_filename, val_df_label))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_df_filename, test_df_label))\n",
        "print(train_ds.element_spec)\n",
        "print(val_ds.element_spec)\n",
        "print(test_ds.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsEfovDVAHGY",
        "outputId": "9aaad61d-216b-4f3d-da20-0e5a23c2c785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "def load_wav_for_map(filename, label):\n",
        "  return load_wav_16k_mono(filename), label\n",
        "\n",
        "train_ds = train_ds.map(load_wav_for_map)\n",
        "print(train_ds.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODu_MEhaj6BB",
        "outputId": "31e62d6d-5ab8-44a4-8732-a608f419d054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "val_ds = val_ds.map(load_wav_for_map)\n",
        "print(val_ds.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm0eamljj53M",
        "outputId": "0806eb0a-ed83-40a9-d8e8-c7e9ef13ce6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "test_ds = test_ds.map(load_wav_for_map)\n",
        "print(test_ds.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0tG8DBNAHcE",
        "outputId": "a7ca6f37-2700-46e4-f728-52cd291a2466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "# applies the embedding extraction model to a wav data\n",
        "def extract_embedding(wav_data, label):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
        "  num_embeddings = tf.shape(embeddings)[0]\n",
        "  return (embeddings, tf.repeat(label, num_embeddings))\n",
        "\n",
        "# extract embedding\n",
        "train_ds = train_ds.map(extract_embedding).unbatch()\n",
        "val_ds = val_ds.map(extract_embedding).unbatch()\n",
        "test_ds = test_ds.map(extract_embedding).unbatch()\n",
        "print(train_ds.element_spec)\n",
        "print(val_ds.element_spec)\n",
        "print(test_ds.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvS2arXt1jIH"
      },
      "source": [
        "### Split the data\n",
        "Use this if you don't split the dataframe in the first place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKVNzIBK1e7g"
      },
      "outputs": [],
      "source": [
        "# # Shuffle the dataset\n",
        "# cached_ds = main_ds.cache()\n",
        "# # shuffled_ds = main_ds.shuffle(buffer_size=2060)\n",
        "\n",
        "# # Determine the size of each split\n",
        "# total_size = len(list(cached_ds))\n",
        "# print(total_size)\n",
        "# train_size = int(0.7 * total_size)\n",
        "# val_size = int(0.2 * total_size)\n",
        "# # test_size is the remaining elements\n",
        "\n",
        "# # Split the data\n",
        "# train_ds = cached_ds.take(train_size)\n",
        "# val_ds = cached_ds.skip(train_size).take(val_size)\n",
        "# test_ds = cached_ds.skip(train_size + val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZYvlFiVsffC"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5PaMwvtcAIe"
      },
      "source": [
        "## Create your model\n",
        "\n",
        "You did most of the work!\n",
        "Next, define a very simple [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) model with one hidden layer and two outputs to recognize cats and dogs from sounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYCE0Fr1GpN3",
        "outputId": "6033a471-0d3f-4f5b-9f8d-2f6f9e54eccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"yamnet_last_layer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 2056      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 658,184\n",
            "Trainable params: 658,184\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classes_name = list(train_metadata['alt_class'].unique())\n",
        "yamnet_last_layer = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32, name='input_embedding'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(classes_name))\n",
        "], name='yamnet_last_layer')\n",
        "\n",
        "yamnet_last_layer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = '/content/drive/MyDrive/Colab Notebooks/Bioakustik-Gajah'\n",
        "MODEL_CHECKPOINT_DIR = os.path.join(SAVE_DIR, 'model-checkpoint/')\n",
        "MODEL_DIR = os.path.join(SAVE_DIR, 'model/')"
      ],
      "metadata": {
        "id": "aaZznHdtApVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qgH35HY0SE"
      },
      "outputs": [],
      "source": [
        "def scheduler(epoch, learning_rate):\n",
        "    if epoch < 5:\n",
        "        return learning_rate\n",
        "    else:\n",
        "        return learning_rate * tf.math.exp(-0.1)\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=MODEL_CHECKPOINT_DIR,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "yamnet_last_layer.save_weights(MODEL_CHECKPOINT_DIR.format(epoch=0))\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "callbacks = [lr_scheduler, earlystopping, model_checkpoint]\n",
        "\n",
        "\n",
        "# initial_lr = 0.1\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=initial_lr,\n",
        "#     decay_steps=500,\n",
        "#     decay_rate=0.96,\n",
        "#     staircase=True\n",
        "# )\n",
        "yamnet_last_layer.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3sj84eOZ3pk",
        "outputId": "62f27db2-2e08-4132-8bc9-abe9497639c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "489/489 [==============================] - 7s 12ms/step - loss: 0.7211 - accuracy: 0.8121 - val_loss: 1.1649 - val_accuracy: 0.7201 - lr: 0.0100\n",
            "Epoch 2/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.6954 - accuracy: 0.7925 - val_loss: 1.1443 - val_accuracy: 0.7255 - lr: 0.0100\n",
            "Epoch 3/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.7251 - accuracy: 0.7944 - val_loss: 0.9677 - val_accuracy: 0.7255 - lr: 0.0100\n",
            "Epoch 4/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.8185 - accuracy: 0.7913 - val_loss: 0.9939 - val_accuracy: 0.7269 - lr: 0.0100\n",
            "Epoch 5/200\n",
            "489/489 [==============================] - 6s 13ms/step - loss: 0.9700 - accuracy: 0.7975 - val_loss: 0.9885 - val_accuracy: 0.7296 - lr: 0.0100\n",
            "Epoch 6/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.6534 - accuracy: 0.8029 - val_loss: 0.9632 - val_accuracy: 0.7038 - lr: 0.0090\n",
            "Epoch 7/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.6470 - accuracy: 0.8025 - val_loss: 0.9386 - val_accuracy: 0.7391 - lr: 0.0082\n",
            "Epoch 8/200\n",
            "489/489 [==============================] - 6s 11ms/step - loss: 0.6201 - accuracy: 0.8134 - val_loss: 0.9862 - val_accuracy: 0.7092 - lr: 0.0074\n",
            "Epoch 9/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.6018 - accuracy: 0.8135 - val_loss: 0.9314 - val_accuracy: 0.7296 - lr: 0.0067\n",
            "Epoch 10/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.5819 - accuracy: 0.8169 - val_loss: 0.9067 - val_accuracy: 0.7473 - lr: 0.0061\n",
            "Epoch 11/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.5676 - accuracy: 0.8244 - val_loss: 0.9935 - val_accuracy: 0.7337 - lr: 0.0055\n",
            "Epoch 12/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.5552 - accuracy: 0.8250 - val_loss: 0.9161 - val_accuracy: 0.7378 - lr: 0.0050\n",
            "Epoch 13/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.5412 - accuracy: 0.8263 - val_loss: 0.9186 - val_accuracy: 0.7500 - lr: 0.0045\n",
            "Epoch 14/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.5335 - accuracy: 0.8327 - val_loss: 0.9007 - val_accuracy: 0.7486 - lr: 0.0041\n",
            "Epoch 15/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.5183 - accuracy: 0.8301 - val_loss: 0.9588 - val_accuracy: 0.7500 - lr: 0.0037\n",
            "Epoch 16/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.5046 - accuracy: 0.8352 - val_loss: 0.9442 - val_accuracy: 0.7473 - lr: 0.0033\n",
            "Epoch 17/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.5016 - accuracy: 0.8358 - val_loss: 0.9319 - val_accuracy: 0.7527 - lr: 0.0030\n",
            "Epoch 18/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4930 - accuracy: 0.8391 - val_loss: 0.9401 - val_accuracy: 0.7500 - lr: 0.0027\n",
            "Epoch 19/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4895 - accuracy: 0.8368 - val_loss: 0.9515 - val_accuracy: 0.7446 - lr: 0.0025\n",
            "Epoch 20/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4881 - accuracy: 0.8413 - val_loss: 0.8985 - val_accuracy: 0.7486 - lr: 0.0022\n",
            "Epoch 21/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4775 - accuracy: 0.8419 - val_loss: 0.9372 - val_accuracy: 0.7527 - lr: 0.0020\n",
            "Epoch 22/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.4698 - accuracy: 0.8430 - val_loss: 0.9568 - val_accuracy: 0.7500 - lr: 0.0018\n",
            "Epoch 23/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4707 - accuracy: 0.8426 - val_loss: 0.9242 - val_accuracy: 0.7486 - lr: 0.0017\n",
            "Epoch 24/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4726 - accuracy: 0.8445 - val_loss: 0.9197 - val_accuracy: 0.7541 - lr: 0.0015\n",
            "Epoch 25/200\n",
            "489/489 [==============================] - 6s 13ms/step - loss: 0.4670 - accuracy: 0.8445 - val_loss: 0.9597 - val_accuracy: 0.7527 - lr: 0.0014\n",
            "Epoch 26/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4580 - accuracy: 0.8437 - val_loss: 0.9415 - val_accuracy: 0.7514 - lr: 0.0012\n",
            "Epoch 27/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.4525 - accuracy: 0.8479 - val_loss: 0.9370 - val_accuracy: 0.7473 - lr: 0.0011\n",
            "Epoch 28/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.4550 - accuracy: 0.8485 - val_loss: 0.9343 - val_accuracy: 0.7391 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4532 - accuracy: 0.8464 - val_loss: 0.9553 - val_accuracy: 0.7500 - lr: 9.0718e-04\n",
            "Epoch 30/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4583 - accuracy: 0.8468 - val_loss: 0.9696 - val_accuracy: 0.7473 - lr: 8.2085e-04\n",
            "Epoch 31/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4530 - accuracy: 0.8463 - val_loss: 0.9939 - val_accuracy: 0.7446 - lr: 7.4273e-04\n",
            "Epoch 32/200\n",
            "489/489 [==============================] - 6s 13ms/step - loss: 0.4446 - accuracy: 0.8493 - val_loss: 0.9671 - val_accuracy: 0.7473 - lr: 6.7205e-04\n",
            "Epoch 33/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4412 - accuracy: 0.8497 - val_loss: 0.9485 - val_accuracy: 0.7473 - lr: 6.0810e-04\n",
            "Epoch 34/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4404 - accuracy: 0.8497 - val_loss: 0.9549 - val_accuracy: 0.7541 - lr: 5.5023e-04\n",
            "Epoch 35/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4457 - accuracy: 0.8500 - val_loss: 0.9691 - val_accuracy: 0.7446 - lr: 4.9787e-04\n",
            "Epoch 36/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4410 - accuracy: 0.8505 - val_loss: 0.9629 - val_accuracy: 0.7473 - lr: 4.5049e-04\n",
            "Epoch 37/200\n",
            "489/489 [==============================] - 6s 11ms/step - loss: 0.4332 - accuracy: 0.8507 - val_loss: 0.9865 - val_accuracy: 0.7473 - lr: 4.0762e-04\n",
            "Epoch 38/200\n",
            "489/489 [==============================] - 6s 13ms/step - loss: 0.4435 - accuracy: 0.8469 - val_loss: 0.9765 - val_accuracy: 0.7486 - lr: 3.6883e-04\n",
            "Epoch 39/200\n",
            "489/489 [==============================] - 6s 12ms/step - loss: 0.4414 - accuracy: 0.8472 - val_loss: 0.9853 - val_accuracy: 0.7459 - lr: 3.3373e-04\n",
            "Epoch 40/200\n",
            "489/489 [==============================] - 7s 14ms/step - loss: 0.4460 - accuracy: 0.8452 - val_loss: 0.9932 - val_accuracy: 0.7473 - lr: 3.0197e-04\n",
            "Epoch 41/200\n",
            "489/489 [==============================] - 6s 11ms/step - loss: 0.4385 - accuracy: 0.8498 - val_loss: 0.9872 - val_accuracy: 0.7514 - lr: 2.7324e-04\n",
            "Epoch 42/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4416 - accuracy: 0.8510 - val_loss: 1.0090 - val_accuracy: 0.7500 - lr: 2.4723e-04\n",
            "Epoch 43/200\n",
            "489/489 [==============================] - 7s 13ms/step - loss: 0.4361 - accuracy: 0.8498 - val_loss: 1.0017 - val_accuracy: 0.7500 - lr: 2.2371e-04\n",
            "Epoch 44/200\n",
            "489/489 [==============================] - 7s 15ms/step - loss: 0.4438 - accuracy: 0.8480 - val_loss: 0.9980 - val_accuracy: 0.7500 - lr: 2.0242e-04\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "batch_size = 16\n",
        "epochs=200\n",
        "validation_steps = len(val_df_filename) // batch_size\n",
        "history = yamnet_last_layer.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbraYKYpdoE"
      },
      "source": [
        "Let's run the `evaluate` method on the test data just to be sure there's no overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Nh5nec3Sky",
        "outputId": "e71e2287-d24f-4a13-9b66-82a01d7d8022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 33s 444ms/step - loss: 0.8480 - accuracy: 0.8117\n",
            "Loss:  0.8479685187339783\n",
            "Accuracy:  0.8117117285728455\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = yamnet_last_layer.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEshxVv45eW0"
      },
      "source": [
        "### Measures Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvWYHms637Se",
        "outputId": "3a43692f-808b-4383-853b-ab167efb09a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8117117117117117\n",
            "Precision: 0.4891850336118439\n",
            "Recall: 0.38766384124077613\n",
            "F1: 0.40755509025420433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "def get_metric_results(model, data_test: tf.data.Dataset.prefetch):\n",
        "    prediction = model.predict(data_test)\n",
        "    y_pred = np.argmax(prediction, axis=1)\n",
        "    y_true = []\n",
        "    for _, label in data_test:\n",
        "        y_true.extend(label.numpy())\n",
        "    y_true = np.array(y_true)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(\"Accuracy: {}\".format(acc))\n",
        "    print(\"Precision: {}\".format(precision))\n",
        "    print(\"Recall: {}\".format(recall))\n",
        "    print(\"F1: {}\".format(f1))\n",
        "\n",
        "get_metric_results(yamnet_last_layer, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yleeev645r"
      },
      "source": [
        "## Save a model that can directly take a WAV file as input\n",
        "\n",
        "Your model works when you give it the embeddings as input.\n",
        "\n",
        "In a real-world scenario, you'll want to use audio data as a direct input.\n",
        "\n",
        "To do that, you will combine YAMNet with your model into a single model that you can export for other applications.\n",
        "\n",
        "To make it easier to use the model's result, the final layer will be a `reduce_mean` operation. When using this model for serving (which you will learn about later in the tutorial), you will need the name of the final layer. If you don't define one, TensorFlow will auto-define an incremental one that makes it hard to test, as it will keep changing every time you train the model. When using a raw TensorFlow operation, you can't assign a name to it. To address this issue, you'll create a custom layer that applies `reduce_mean` and call it `'classifier'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUVCI2Suunpw"
      },
      "outputs": [],
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_Npm0nzlwc"
      },
      "outputs": [],
      "source": [
        "saved_model_path = os.path.join(DATA_DIR, MODEL_DIR)\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
        "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle, trainable=False, name='yamnet')\n",
        "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
        "serving_outputs = yamnet_last_layer(embeddings_output)\n",
        "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "y-0bY5FMme1C",
        "outputId": "fc4bf385-e9ad-4b09-fe6b-ebd8b1621fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "serving_model.save(saved_model_path, include_optimizer=True)\n",
        "tf.keras.utils.plot_model(serving_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btHQDN9mqxM_"
      },
      "source": [
        "Load your saved model to verify that it works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkYVpJS72WWB"
      },
      "outputs": [],
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BkmvvNzq49l"
      },
      "source": [
        "And for the final test: given some sound data, does your model return the correct result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "LD-dhNpFrFkF",
        "outputId": "158bb365-df43-43de-c7d6-628cfe6a5882"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              filename alt_class  alt_class_id\n",
              "137  /content/drive/MyDrive/Colab Notebooks/dataset...     growl             5\n",
              "170  /content/drive/MyDrive/Colab Notebooks/dataset...    rumble             2\n",
              "90   /content/drive/MyDrive/Colab Notebooks/dataset...     other             1\n",
              "121  /content/drive/MyDrive/Colab Notebooks/dataset...     other             1\n",
              "136  /content/drive/MyDrive/Colab Notebooks/dataset...     growl             5\n",
              "101  /content/drive/MyDrive/Colab Notebooks/dataset...      bark             7\n",
              "122  /content/drive/MyDrive/Colab Notebooks/dataset...     other             1\n",
              "191  /content/drive/MyDrive/Colab Notebooks/dataset...     other             1\n",
              "172  /content/drive/MyDrive/Colab Notebooks/dataset...     other             1\n",
              "62   /content/drive/MyDrive/Colab Notebooks/dataset...   trumpet             0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60767af0-e2f7-44e8-9c8d-b4511e70bd3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>alt_class</th>\n",
              "      <th>alt_class_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>growl</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>rumble</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>growl</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>bark</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/dataset...</td>\n",
              "      <td>trumpet</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60767af0-e2f7-44e8-9c8d-b4511e70bd3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60767af0-e2f7-44e8-9c8d-b4511e70bd3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60767af0-e2f7-44e8-9c8d-b4511e70bd3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "test_metadata[['filename', 'alt_class', 'alt_class_id']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeXtD5HO28y-",
        "outputId": "adc91da0-95eb-447b-fe33-d0db34e1c3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction sound is: other, confident value: 0.748458743095398\n"
          ]
        }
      ],
      "source": [
        "testing_wav_data = load_wav_16k_mono(test_metadata['filename'][49])\n",
        "\n",
        "reloaded_results = reloaded_model(testing_wav_data)\n",
        "cls = tf.math.argmax(reloaded_results)\n",
        "prediction_class = classes_name[cls]\n",
        "prediction_prob = tf.nn.softmax(reloaded_results, axis=-1)\n",
        "cls_prob = prediction_prob[cls]\n",
        "print(f\"The prediction sound is: {prediction_class}, confident value: {cls_prob}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRrOcBYTUgwn"
      },
      "source": [
        "If you want to try your new model on a serving setup, you can use the 'serving_default' signature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycC8zzDSUG2s",
        "outputId": "7675d92d-d48c-458d-f43b-2feadef53d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main sound is: other\n"
          ]
        }
      ],
      "source": [
        "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
        "pred_results = classes_name[tf.math.argmax(serving_results['classifier'])]\n",
        "print(f'The main sound is: {pred_results}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Model to TF-Lite"
      ],
      "metadata": {
        "id": "kYM4cnATOd-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBnlNaL10MCm"
      },
      "outputs": [],
      "source": [
        "model = tf.saved_model.load(export_dir=os.path.join(SAVE_DIR, 'model'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_wav_data = load_wav_16k_mono(test_metadata['filename'][49])\n",
        "\n",
        "reloaded_results = model(testing_wav_data)\n",
        "cls = tf.math.argmax(reloaded_results)\n",
        "classes_name = list(train_metadata['alt_class'].unique())\n",
        "prediction_class = classes_name[cls]\n",
        "prediction_prob = tf.nn.softmax(reloaded_results, axis=-1)\n",
        "cls_prob = prediction_prob[cls]\n",
        "print(f\"The prediction sound is: {prediction_class}, confident value: {cls_prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g-mQTuaqbbn",
        "outputId": "7086b0d7-2c3e-4798-d4c2-38e1b46dfa63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction sound is: other, confident value: 0.748458743095398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(SAVE_DIR, 'model')) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "# tflite_dir = os.path.join(SAVE_DIR, 'TFLite-model')\n",
        "with open('model-v0.2.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "VusQGg_8rUcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model from the reloaded_model object\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(reloaded_model)\n",
        "tf_lite_model = converter.convert()\n",
        "\n",
        "with open('model-v-0.2-from-keras-model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "3jB-3WITtfbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXks8ZOMvLC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Cb4espuLKJiA"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}